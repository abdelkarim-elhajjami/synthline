import pandas as pd
import json
from pathlib import Path
from typing import Dict, Tuple
import logging
from sklearn.model_selection import train_test_split
from config import PathConfig

logger = logging.getLogger(__name__)

class DataProcessor:
    """Handles all data processing, organization, and statistics."""
    
    LABEL_MAP = {
        'Ambiguous': 0,
        'Directive': 1,
        'Non-Atomic': 2,
        'Non-Measurable': 3,
        'Optional': 4,
        'Uncertain': 5
    }
    
    def __init__(self, output_dir: Path = Path("train_classifiers/datasets")):
        self._output_dir = output_dir  # Make private since it's internal
        self._output_dir.mkdir(parents=True, exist_ok=True)
        self._stats = {}  # Make private since it's internal state
    
    def _clean_and_prepare(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and prepare dataset"""
        # Standardize column names
        df.columns = [c.lower() for c in df.columns]
        if 'requirement_text' in df.columns:
            df = df.rename(columns={'requirement_text': 'requirement'})
            
        # Clean requirement text
        df['requirement'] = (df['requirement']
                           .str.replace('"""', '', regex=False)
                           .str.replace('"', '', regex=False)
                           .str.strip())
        
        # Encode labels
        if 'label' in df.columns:
            df = df.rename(columns={'label': 'class'})
        df['class'] = df['class'].map(self.LABEL_MAP)
        
        # Remove duplicates
        df = df.drop_duplicates(subset=['requirement'])
        
        return df
    
    def _calculate_stats(self, df: pd.DataFrame, name: str):
        """Calculate and store statistics for a dataset."""
        self._stats[name] = {
            'total_samples': len(df),
            'samples_per_class': df['class'].value_counts().to_dict(),
            'class_distribution': (df['class'].value_counts(normalize=True) * 100).to_dict()
        }
    
    def load_synthetic_data(self, synthline_output_dir: Path, llm: str) -> pd.DataFrame:
        """Load synthetic data generated by synthline for a specific LLM."""
        llm_dir = synthline_output_dir / llm
        if not llm_dir.exists():
            raise FileNotFoundError(f"No synthline output directory found for {llm} at {llm_dir}")
            
        dfs = []
        for file in llm_dir.glob("*.csv"):
            df = pd.read_csv(file)
            df['llm'] = llm
            dfs.append(df)
            
        if not dfs:
            raise FileNotFoundError(f"No synthetic data CSV files found in {llm_dir}")
            
        combined_df = pd.concat(dfs, ignore_index=True)
        combined_df = self._clean_and_prepare(combined_df)
        
        return combined_df
    
    def load_real_data(self, filepath: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load and split real dataset."""
        df = pd.read_csv(filepath, sep=';')
        df = self._clean_and_prepare(df)
        
        train_df, test_df = train_test_split(
            df, test_size=0.30, random_state=42,
            stratify=df['class']
        )
        
        return train_df, test_df
    
    def save_datasets(self, datasets: Dict[str, pd.DataFrame]):
        """Save all datasets and their statistics."""
        # Save datasets
        for name, df in datasets.items():
            output_path = self._output_dir / f"{name}.csv"
            df.to_csv(output_path, index=False, sep=';')
            self._calculate_stats(df, name)
        
        # Save statistics
        stats_path = self._output_dir / 'datasets_stats.json'
        with open(stats_path, 'w') as f:
            json.dump(self._stats, f, indent=2)

def main():
    paths = PathConfig()
    project_root = Path(__file__).parent.parent
    processor = DataProcessor(project_root / paths.datasets_dir)
    
    try:
        # Load synthetic data generated by synthline for each LLM
        logger.info("Loading synthetic datasets from synthline output...")
        deepseek = processor.load_synthetic_data(
            project_root / paths.input_dir,  # This should point to synthline's output dir
            'deepseek-chat'
        )
        gpt4o = processor.load_synthetic_data(
            project_root / paths.input_dir, 
            'gpt-4o'
        )
        
        # Create combined synthetic dataset
        gpt4o_deepseek = pd.concat([deepseek, gpt4o])
        
        # Load real dataset for training/testing
        logger.info("Loading and splitting real dataset for evaluation...")
        real_train, real_test = processor.load_real_data(
            project_root / paths.datasets_dir / 'real_dataset.csv'
        )
        
        # Create hybrid datasets (combining real + synthetic)
        logger.info("Creating hybrid datasets (real + synthetic)...")
        real_train_deepseek = pd.concat([deepseek, real_train])
        real_train_gpt4o = pd.concat([gpt4o, real_train])
        real_train_gpt4o_deepseek = pd.concat([gpt4o_deepseek, real_train])
        
        # Save all datasets and generate statistics
        logger.info("Saving processed datasets and generating statistics...")
        datasets = {
            'deepseek': deepseek,
            'gpt4o': gpt4o,
            'gpt4o_deepseek': gpt4o_deepseek,
            'real_train': real_train,
            'real_test': real_test,
            'real_train_deepseek': real_train_deepseek,
            'real_train_gpt4o': real_train_gpt4o,
            'real_train_gpt4o_deepseek': real_train_gpt4o_deepseek
        }
        
        processor.save_datasets(datasets)
        logger.info("Dataset preparation completed successfully!")
        
    except Exception as e:
        logger.error(f"Error during dataset preparation: {str(e)}")
        raise

if __name__ == "__main__":
    main()